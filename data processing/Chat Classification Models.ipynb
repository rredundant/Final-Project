{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jenniferwilson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query used to get data from Hive to get raw data from Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran this in Hadoop on WordPress.com's servers: https://mc.a8c.com/pb/213b7/#plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13312, 12)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each month has a different CSV of data. Here we programatically combine them into one dataframe.\n",
    "\n",
    "files = glob.glob('Data/*.csv')\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in files:\n",
    "    num_lines = sum(1 for l in open(filename))\n",
    "    size = int(num_lines / 6 ) # use these values: 3,4,5,6\n",
    "    skip_idx = random.sample(range(1, num_lines), num_lines - size)\n",
    "    df = pd.read_csv(filename, skiprows=skip_idx, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# Shuffle the data.\n",
    "df = shuffle(df)\n",
    "\n",
    "# Confirm size of dataset.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Business    5022\n",
       "Premium     4718\n",
       "Personal    3572\n",
       "Name: plan_purchased_nice, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class balance. Looks pretty balanced!\n",
    "\n",
    "df['plan_purchased_nice'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords so as to clean up our features (vectorized text).\n",
    "stop = set(stopwords.words('english'))\n",
    "df['msg_whole_clean'] = df['msg_whole'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features out of text in chat transcripts.\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 6), analyzer ='word', max_df =.75, min_df = .05) \n",
    "\n",
    "features = vectorizer.fit_transform(df['msg_whole_clean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test segments.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "         features, df['plan_purchased_nice'], test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Accuracy: 40.1142%\n",
      "Log Loss: 5.28079487964\n",
      "==============================\n",
      "LinearDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 45.6130%\n",
      "Log Loss: 1.04323102642\n",
      "==============================\n",
      "QuadraticDiscriminantAnalysis\n",
      "****Results****\n",
      "Accuracy: 39.2127%\n",
      "Log Loss: 18.678103331\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Run data through various classifiers to find the highest accuracy.\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    CalibratedClassifierCV(LinearSVC()),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "# Logging for visual comparison (optional)\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    \n",
    "    # For the last three classifiers above to run, we need to convert \n",
    "    # the sparse matrix generated from the countvectorizer step above\n",
    "    # into a dense matrix.\n",
    "    X_train = csr_matrix(X_train).todense()\n",
    "    X_test = csr_matrix(X_test).todense()\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Results****\n",
      "Learning Rate: 100.0000%\n",
      "Accuracy: 43.6298%\n",
      "==============================\n",
      "****Results****\n",
      "Learning Rate: 50.0000%\n",
      "Accuracy: 44.5312%\n",
      "==============================\n",
      "****Results****\n",
      "Learning Rate: 25.0000%\n",
      "Accuracy: 45.9435%\n",
      "==============================\n",
      "****Results****\n",
      "Learning Rate: 10.0000%\n",
      "Accuracy: 46.7548%\n",
      "==============================\n",
      "****Results****\n",
      "Learning Rate: 5.0000%\n",
      "Accuracy: 47.4459%\n",
      "==============================\n",
      "****Results****\n",
      "Learning Rate: 1.0000%\n",
      "Accuracy: 46.6647%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "#Start individually optimizing hyperparameters of highest performing algorithm: GradientBoostingClassifier.\n",
    "\n",
    "# Experiment with different learning rates.\n",
    "learning_rates = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\n",
    "print('****Results****')\n",
    "\n",
    "for eta in learning_rates:\n",
    "    clf = GradientBoostingClassifier(learning_rate=eta)\n",
    "\n",
    "    X_train = csr_matrix(X_train).todense()\n",
    "    X_test = csr_matrix(X_test).todense()\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Learning Rate: {:.4%}\".format(eta))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Results****\n",
      "N estimators: 1\n",
      "Accuracy: 44.4712%\n",
      "==============================\n",
      "N estimators: 2\n",
      "Accuracy: 44.8918%\n",
      "==============================\n",
      "N estimators: 4\n",
      "Accuracy: 44.9219%\n",
      "==============================\n",
      "N estimators: 8\n",
      "Accuracy: 46.6647%\n",
      "==============================\n",
      "N estimators: 16\n",
      "Accuracy: 46.9351%\n",
      "==============================\n",
      "N estimators: 32\n",
      "Accuracy: 47.4159%\n",
      "==============================\n",
      "N estimators: 64\n",
      "Accuracy: 47.1454%\n",
      "==============================\n",
      "N estimators: 100\n",
      "Accuracy: 47.2055%\n",
      "==============================\n",
      "N estimators: 200\n",
      "Accuracy: 45.9435%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different n_estimators.\n",
    "\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "print('****Results****')\n",
    "\n",
    "for estimator in n_estimators:\n",
    "    clf = GradientBoostingClassifier(n_estimators=estimator)\n",
    "\n",
    "    X_train = csr_matrix(X_train).todense()\n",
    "    X_test = csr_matrix(X_test).todense()\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"N estimators: {}\".format(estimator))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    print(\"=\"*30)\n",
    "\n",
    "#  Highest: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Results****\n",
      "Max Depth: 1.0\n",
      "Accuracy: 47.3858%\n",
      "==============================\n",
      "Max Depth: 2.0\n",
      "Accuracy: 47.2957%\n",
      "==============================\n",
      "Max Depth: 3.0\n",
      "Accuracy: 46.7548%\n",
      "==============================\n",
      "Max Depth: 4.0\n",
      "Accuracy: 46.3642%\n",
      "==============================\n",
      "Max Depth: 5.0\n",
      "Accuracy: 45.9736%\n",
      "==============================\n",
      "Max Depth: 6.0\n",
      "Accuracy: 46.1238%\n",
      "==============================\n",
      "Max Depth: 7.0\n",
      "Accuracy: 45.2224%\n",
      "==============================\n",
      "Max Depth: 8.0\n",
      "Accuracy: 44.7716%\n",
      "==============================\n",
      "Max Depth: 9.0\n",
      "Accuracy: 44.3810%\n",
      "==============================\n",
      "Max Depth: 10.0\n",
      "Accuracy: 44.7416%\n",
      "==============================\n",
      "Max Depth: 11.0\n",
      "Accuracy: 44.0805%\n",
      "==============================\n",
      "Max Depth: 12.0\n",
      "Accuracy: 43.7500%\n",
      "==============================\n",
      "Max Depth: 13.0\n",
      "Accuracy: 43.3293%\n",
      "==============================\n",
      "Max Depth: 14.0\n",
      "Accuracy: 43.8702%\n",
      "==============================\n",
      "Max Depth: 15.0\n",
      "Accuracy: 43.2692%\n",
      "==============================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-67eaa439ec0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1463\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n\u001b[1;32m   1464\u001b[0m                                     \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1527\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1528\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1194\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment with different max_depths.\n",
    "\n",
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "\n",
    "print('****Results****')\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    clf = GradientBoostingClassifier(max_depth=max_depth)\n",
    "\n",
    "    X_train = csr_matrix(X_train).todense()\n",
    "    X_test = csr_matrix(X_test).todense()\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Max Depth: {}\".format(max_depth))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Results****\n",
      "Max Samples Split: 0.1\n",
      "Accuracy: 47.7464%\n",
      "==============================\n",
      "Max Samples Split: 0.2\n",
      "Accuracy: 48.0769%\n",
      "==============================\n",
      "Max Samples Split: 0.3\n",
      "Accuracy: 47.6562%\n",
      "==============================\n",
      "Max Samples Split: 0.4\n",
      "Accuracy: 47.7764%\n",
      "==============================\n",
      "Max Samples Split: 0.5\n",
      "Accuracy: 47.7163%\n",
      "==============================\n",
      "Max Samples Split: 0.6\n",
      "Accuracy: 47.8365%\n",
      "==============================\n",
      "Max Samples Split: 0.7\n",
      "Accuracy: 48.0469%\n",
      "==============================\n",
      "Max Samples Split: 0.8\n",
      "Accuracy: 47.5361%\n",
      "==============================\n",
      "Max Samples Split: 0.9\n",
      "Accuracy: 47.3858%\n",
      "==============================\n",
      "Max Samples Split: 1.0\n",
      "Accuracy: 47.3858%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different min_samples_splits.\n",
    "\n",
    "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "\n",
    "print('****Results****')\n",
    "\n",
    "for min_samples_split in min_samples_splits:\n",
    "    clf = GradientBoostingClassifier(min_samples_split=min_samples_split)\n",
    "    \n",
    "    X_train = csr_matrix(X_train).todense()\n",
    "    X_test = csr_matrix(X_test).todense()\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Max Samples Split: {}\".format(min_samples_split))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    print(\"=\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Samples Leafs: 0.1\n",
      "Accuracy: 41.5865%\n",
      "==============================\n",
      "Max Samples Leafs: 0.2\n",
      "Accuracy: 40.0841%\n",
      "==============================\n",
      "Max Samples Leafs: 0.3\n",
      "Accuracy: 39.5733%\n",
      "==============================\n",
      "Max Samples Leafs: 0.4\n",
      "Accuracy: 39.5433%\n",
      "==============================\n",
      "Max Samples Leafs: 0.5\n",
      "Accuracy: 38.2212%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different min_samples_leafs.\n",
    "\n",
    "min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "\n",
    "for min_samples_leaf in min_samples_leafs:\n",
    "    clf = GradientBoostingClassifier(min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    X_train = csr_matrix(X_train).todense()\n",
    "    X_test = csr_matrix(X_test).todense()\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Max Samples Leafs: {}\".format(min_samples_leaf))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features: 1\n",
      "Accuracy: 45.7031%\n",
      "==============================\n",
      "Max Features: 2\n",
      "Accuracy: 45.8233%\n",
      "==============================\n",
      "Max Features: 3\n",
      "Accuracy: 46.7548%\n",
      "==============================\n",
      "Max Features: 4\n",
      "Accuracy: 47.4760%\n",
      "==============================\n",
      "Max Features: 5\n",
      "Accuracy: 46.7849%\n",
      "==============================\n",
      "Max Features: 6\n",
      "Accuracy: 46.8450%\n",
      "==============================\n",
      "Max Features: 7\n",
      "Accuracy: 47.2356%\n",
      "==============================\n",
      "Max Features: 8\n",
      "Accuracy: 47.1454%\n",
      "==============================\n",
      "Max Features: 9\n",
      "Accuracy: 46.9952%\n",
      "==============================\n",
      "Max Features: 10\n",
      "Accuracy: 47.0252%\n",
      "==============================\n",
      "Max Features: 11\n",
      "Accuracy: 47.2356%\n",
      "==============================\n",
      "Max Features: 12\n",
      "Accuracy: 48.0469%\n",
      "==============================\n",
      "Max Features: 13\n",
      "Accuracy: 47.2656%\n",
      "==============================\n",
      "Max Features: 14\n",
      "Accuracy: 47.3257%\n",
      "==============================\n",
      "Max Features: 15\n",
      "Accuracy: 47.7163%\n",
      "==============================\n",
      "Max Features: 16\n",
      "Accuracy: 46.9651%\n",
      "==============================\n",
      "Max Features: 17\n",
      "Accuracy: 47.2356%\n",
      "==============================\n",
      "Max Features: 18\n",
      "Accuracy: 47.5060%\n",
      "==============================\n",
      "Max Features: 19\n",
      "Accuracy: 47.3858%\n",
      "==============================\n",
      "Max Features: 20\n",
      "Accuracy: 47.0553%\n",
      "==============================\n",
      "Max Features: 21\n",
      "Accuracy: 47.5060%\n",
      "==============================\n",
      "Max Features: 22\n",
      "Accuracy: 47.7163%\n",
      "==============================\n",
      "Max Features: 23\n",
      "Accuracy: 47.1454%\n",
      "==============================\n",
      "Max Features: 24\n",
      "Accuracy: 47.4760%\n",
      "==============================\n",
      "Max Features: 25\n",
      "Accuracy: 47.2055%\n",
      "==============================\n",
      "Max Features: 26\n",
      "Accuracy: 46.7849%\n",
      "==============================\n",
      "Max Features: 27\n",
      "Accuracy: 47.4760%\n",
      "==============================\n",
      "Max Features: 28\n",
      "Accuracy: 47.2356%\n",
      "==============================\n",
      "Max Features: 29\n",
      "Accuracy: 47.2055%\n",
      "==============================\n",
      "Max Features: 30\n",
      "Accuracy: 47.1154%\n",
      "==============================\n",
      "Max Features: 31\n",
      "Accuracy: 46.9050%\n",
      "==============================\n",
      "Max Features: 32\n",
      "Accuracy: 47.1454%\n",
      "==============================\n",
      "Max Features: 33\n",
      "Accuracy: 47.2055%\n",
      "==============================\n",
      "Max Features: 34\n",
      "Accuracy: 47.1154%\n",
      "==============================\n",
      "Max Features: 35\n",
      "Accuracy: 47.3558%\n",
      "==============================\n",
      "Max Features: 36\n",
      "Accuracy: 47.0553%\n",
      "==============================\n",
      "Max Features: 37\n",
      "Accuracy: 47.1154%\n",
      "==============================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-2d870aed206d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1463\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n\u001b[1;32m   1464\u001b[0m                                     \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1527\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1528\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1194\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/PythonData/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment with different max_features.\n",
    "\n",
    "max_features = list(range(1,features.shape[1]))\n",
    "\n",
    "for max_feature in max_features:\n",
    "    clf = GradientBoostingClassifier(max_features=max_feature)\n",
    "    \n",
    "    X_train = csr_matrix(X_train).todense()\n",
    "    X_test = csr_matrix(X_test).todense()\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Max Features: {}\".format(max_feature))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.3077%\n"
     ]
    }
   ],
   "source": [
    "# By combining the individual optimizations above into one algorithm\n",
    "# we can see the impact this has on accuracy: 42.3077%.\n",
    "# That's worse than the classifier with NO hyperparameters (see next cell).\n",
    "\n",
    "clf = GradientBoostingClassifier(learning_rate = 0.5,\n",
    "                                    n_estimators = 32,\n",
    "                                    max_features = 12,\n",
    "                                    min_samples_split = 0.7,\n",
    "                                    min_samples_leaf = 0.1)\n",
    "    \n",
    "X_train = csr_matrix(X_train).todense()\n",
    "X_test = csr_matrix(X_test).todense()\n",
    "clf.fit(X_train, y_train)\n",
    "name = clf.__class__.__name__\n",
    "\n",
    "train_predictions = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, train_predictions)\n",
    "print(\"Accuracy: {:.4%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.7248%\n"
     ]
    }
   ],
   "source": [
    "# This is the classifier run with no hyperparamters.\n",
    "# Accuracy = 46.7248%\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "    \n",
    "X_train = csr_matrix(X_train).todense()\n",
    "X_test = csr_matrix(X_test).todense()\n",
    "clf.fit(X_train, y_train)\n",
    "name = clf.__class__.__name__\n",
    "\n",
    "train_predictions = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, train_predictions)\n",
    "print(\"Accuracy: {:.4%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.5361%\n"
     ]
    }
   ],
   "source": [
    "# Finally, we tested optimizing each of the hyperparamters below\n",
    "# one by one, adding a new parameter each time to the highest performing\n",
    "# from the previous run.\n",
    "# Accuracy = 47.5361%\n",
    "\n",
    "clf = GradientBoostingClassifier(learning_rate =.5,\n",
    "                                 n_estimators = 8,\n",
    "                                 max_depth = 2,\n",
    "                                 min_samples_split = 0.2,\n",
    "                                 max_features = 35)\n",
    "#                                      min_samples_leaf \n",
    "\n",
    "X_train = csr_matrix(X_train).todense()\n",
    "X_test = csr_matrix(X_test).todense()\n",
    "clf.fit(X_train, y_train)\n",
    "name = clf.__class__.__name__\n",
    "\n",
    "train_predictions = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, train_predictions)\n",
    "print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
